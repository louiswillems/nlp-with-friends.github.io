---
permalink: /upcoming/tom-mccoy
title: "Universal Linguistic Inductive Biases via Meta-Learning"
speaker: Tom McCoy
author: Tom McCoy
date: Aug 12, 2020
time: 14:00 UTC
bio: "Tom McCoy is a PhD student at Johns Hopkins. He studies the linguistic abilities of neural networks, focusing on inductive biases and representations of compositional structure."
read_time: true
comments: true
---

<a href="https://lolmythesis.com/" class="one-line">I never meta learning I didn't like.</a>

Despite their impressive scores on NLP leaderboards, current neural models fall short of humans in two major ways: They require massive amounts of training data, and they generalize poorly to novel types of examples. To address these problems, we propose an approach for giving linguistic inductive biases to a model, where *inductive biases* are factors that affect how a learner generalizes. Our approach imparts inductive biases using *meta-learning*, a procedure through which the model discovers how to acquire new languages more quickly via exposure to many possible languages. By controlling the properties of the languages used during meta-learning, we can control the inductive biases that meta-learning imparts. We demonstrate the effectiveness of this approach using a case study from phonology.

<hr>

**Tom McCoy** is a PhD student at Johns Hopkins. He studies the linguistic abilities of neural networks, focusing on inductive biases and representations of compositional structure.

